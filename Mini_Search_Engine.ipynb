{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZePHkrxCVM3",
        "outputId": "49f25cfe-a649-4f95-9654-cfa95836c42b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Mini Search Engine"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import wordnet\n",
        "from spellchecker import SpellChecker\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjGnQZaxC8ZV",
        "outputId": "f6be44b2-c844-4719-9726-468f7d9066bd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPnWVHXzCsOF",
        "outputId": "5aa57978-b26f-474e-d4a8-2111a6878b09"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.2-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SearchEngine():\n",
        "  def __init__(self):\n",
        "    # constructor\n",
        "    self.documents = {} # key is index of document, value is the actual words of the document\n",
        "\n",
        "  def add_document(self, index, content):\n",
        "    self.documents[index] = content\n",
        "\n",
        "  '''def index_document(self, document):\n",
        "    # Documents are keys, values are words contained in that doc\n",
        "    words = self.getstem(document)\n",
        "    for word in words:\n",
        "      if word in self.index:\n",
        "        self.inverted_index[word].append(document)\n",
        "      else:\n",
        "        self.inverted_index[word] = [document]'''\n",
        "\n",
        "  def normalize(self, document):\n",
        "    # tokenize the word\n",
        "    document = document.lower()\n",
        "    tokens = word_tokenize(document) # list of strings\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    # stemmer.stem takes the tokens into its root form (i.e. jumping -> jump, dogs -> dog)\n",
        "\n",
        "    return stemmed_tokens\n",
        "\n",
        "\n",
        "  def search(self, query, numresults=3):\n",
        "      query_words = self.normalize(query)\n",
        "      matching_docs = defaultdict(int)\n",
        "      for index, content in self.documents.items():\n",
        "        doc_words = self.normalize(content)\n",
        "        for word in query_words:\n",
        "          if word in doc_words:\n",
        "            matching_docs[index] += 1\n",
        "\n",
        "      sorted_docs = sorted(matching_docs.items(), key=lambda pair: pair[1], reverse=True)\n",
        "\n",
        "      sorted_docs = sorted_docs[:numresults]\n",
        "      sorted_docs = [item1 for item1, item2 in sorted_docs]\n",
        "      return sorted_docs\n",
        "\n",
        "\n",
        "\n",
        "# Takes in a query, gives us terms that are synonyms to the query automatically without manually having to specify alternative words to the query.\n",
        "class QueryExpander():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def get_syn(self, term):\n",
        "    # find synonym for term\n",
        "    synonyms = set()\n",
        "    wordNetsynonyms = wordnet.synsets(term)\n",
        "    for s in wordNetsynonyms:\n",
        "      for lemma in s.lemmas():\n",
        "        synonyms.add(lemma.name())\n",
        "\n",
        "    return list(synonyms)\n",
        "\n",
        "  def query_expand(self, query):\n",
        "    query_terms = self.normalize(query)\n",
        "    expanded_terms = []\n",
        "    for term in query_terms:\n",
        "      synonyms = self.get_syn(term)\n",
        "      expanded_terms.extend(synonyms) #.extend merges two lists into one list\n",
        "    return \" \".join(expanded_terms)\n",
        "\n",
        "\n",
        "  def normalize(self, query):\n",
        "    query = query.lower()\n",
        "    tokens = word_tokenize(query) # list of strings\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "  # stemmer.stem takes the tokens into its root form (i.e. jumping -> jump, dogs -> dog)\n",
        "\n",
        "    return stemmed_tokens\n",
        "\n",
        "def main():\n",
        "  searchengine = SearchEngine()\n",
        "  searchengine.add_document(\"doc1\", \"NLP for beginners\")\n",
        "  searchengine.add_document(\"doc2\", \"NLP stands for Natural Language Processessing\")\n",
        "  searchengine.add_document(\"doc3\", \"Python Developers like PyTorch\")\n",
        "  searchengine.add_document(\"doc4\", \"dog cat jump\")\n",
        "\n",
        "  queryexpand = QueryExpander()\n",
        "  while True:\n",
        "    userinput = input(\"Enter your query: \")\n",
        "    if userinput.lower() == \"exit\":\n",
        "      break\n",
        "    expandedQuery = queryexpand.query_expand(userinput)\n",
        "    results = searchengine.search(expandedQuery)\n",
        "    print(\" \")\n",
        "    print(\"expandedQuery: \", expandedQuery)\n",
        "    if results:\n",
        "      for i, result in enumerate(results, 1):\n",
        "        print(result)\n",
        "    else:\n",
        "      print(\"No match found.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpV75kk3CzGb",
        "outputId": "46704aaa-f433-4af0-ee36-baeb19ed8999"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "expandedQuery:  natural_language_processing NLP human_language_technology\n",
            "doc1\n",
            "doc2\n",
            " \n",
            "expandedQuery:  pawl weenie give_chase frank heel dog track click hound domestic_dog tail wiener firedog blackguard cad tag frankfurter frump wienerwurst chase_after bounder hot_dog andiron hotdog chase dog-iron trail detent go_after Canis_familiaris\n",
            "doc4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search = SearchEngine()\n",
        "search.add_document(\"doc1\", \"hello i'm troy.\")\n",
        "search.search(\"hello i'm troy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y7Vpp99aH0V",
        "outputId": "32504490-3728-477a-8108-543ee655fd26"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['doc1']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9o6YoOgTaMeT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}